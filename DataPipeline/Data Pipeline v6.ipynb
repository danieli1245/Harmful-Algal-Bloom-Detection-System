{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1799c53f-81e5-474f-9e8f-0dd3a411be2a",
   "metadata": {},
   "source": [
    "#### pip install pandas numpy matplotlib earthaccess netcdf4 scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcca99d-9116-4735-9940-6b0dc3dbcdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import earthaccess\n",
    "import netCDF4 as nc\n",
    "import os\n",
    "from pathlib import Path\n",
    "from scipy.interpolate import griddata\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76cad8d-67fa-462f-8dbf-590e0ba6fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config for MVP\n",
    "DATACUBE_CONFIG = {\n",
    "    'spatial_extent_km': 30,      \n",
    "    'temporal_extent_days': 5,    \n",
    "    'spatial_resolution_km': 2,  \n",
    "    'temporal_resolution_days': 1 \n",
    "}\n",
    "\n",
    "HABNET_MODALITIES = [\n",
    "    'chlor_a'  \n",
    "    # later we can use 'par', 'Rrs_443', 'Rrs_488', 'Rrs_531', 'Rrs_555'\n",
    "]\n",
    "\n",
    "# Gulf of Mexico  \n",
    "GULF_BOUNDS = {\n",
    "    'lat_min': 24.0, 'lat_max': 30.5,\n",
    "    'lon_min': -88.0, 'lon_max': -80.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c8c9d0-43ca-4437-858c-0e03211baa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory structure for data storage\n",
    "def setup_directories():\n",
    "    base_dir = Path(\"habnet_mvp_data\")\n",
    "    raw_dir = base_dir / \"raw_modis_l2\"\n",
    "    processed_dir = base_dir / \"processed_datacubes\"\n",
    "    \n",
    "    for directory in [base_dir, raw_dir, processed_dir]:\n",
    "        directory.mkdir(exist_ok=True)\n",
    "        \n",
    "    return base_dir, raw_dir, processed_dir\n",
    "\n",
    "# get lat/lon around event \n",
    "def calculate_spatial_bounds(lat, lon, extent_km=100):\n",
    "    extent_deg = extent_km / 111.0  # 1 degree is about 111 km\n",
    "    return {\n",
    "        'lat_min': lat - extent_deg/2,\n",
    "        'lat_max': lat + extent_deg/2,\n",
    "        'lon_min': lon - extent_deg/2,\n",
    "        'lon_max': lon + extent_deg/2\n",
    "    }\n",
    "\n",
    "# get temporal bounds\n",
    "def calculate_temporal_bounds(event_date, days_before=10): \n",
    "    end_date = event_date + timedelta(days=1)  # include event day\n",
    "    start_date = event_date - timedelta(days=days_before)\n",
    "    return {'start_date': start_date, 'end_date': end_date}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df2bb40-dd73-4abc-916f-9985366f8d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ground truth data and filter\n",
    "def load_and_filter_hab_events(csv_file='habsos_20240430.csv'):\n",
    "    print(\"Loading ground truth data\")\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['SAMPLE_DATE'] = pd.to_datetime(df['SAMPLE_DATE'])\n",
    "    \n",
    "    # filter for Karenia brevis\n",
    "    kb_data = df[df['SPECIES'] == 'brevis'].copy()\n",
    "    kb_clean = kb_data.dropna(subset=['LATITUDE', 'LONGITUDE', 'SAMPLE_DATE', 'CELLCOUNT']).copy()\n",
    "    \n",
    "    # Positive HAB when Karenia brevis > 50,000 cells/L\n",
    "    # Negative HAB when Karenia brevis = 0 cells/L\n",
    "    positive_events = kb_clean[kb_clean['CELLCOUNT'] > 50000].copy()\n",
    "    positive_events['HAB_EVENT'] = 1\n",
    "    \n",
    "    negative_events = kb_clean[kb_clean['CELLCOUNT'] == 0].copy()\n",
    "    negative_events['HAB_EVENT'] = 0\n",
    "    \n",
    "    hab_events = pd.concat([positive_events, negative_events], ignore_index=True)\n",
    "    \n",
    "    # Gulf of Mexico area\n",
    "    gulf_events = hab_events[\n",
    "        (hab_events['LATITUDE'] >= GULF_BOUNDS['lat_min']) &\n",
    "        (hab_events['LATITUDE'] <= GULF_BOUNDS['lat_max']) &\n",
    "        (hab_events['LONGITUDE'] >= GULF_BOUNDS['lon_min']) &\n",
    "        (hab_events['LONGITUDE'] <= GULF_BOUNDS['lon_max'])\n",
    "    ].copy()\n",
    "    \n",
    "    # filter closer to MODIS date (2003-2018)\n",
    "    modis_start = datetime(2003, 1, 1)\n",
    "    modis_end = datetime(2018, 12, 31)\n",
    "    final_events = gulf_events[\n",
    "        (gulf_events['SAMPLE_DATE'] >= modis_start) &\n",
    "        (gulf_events['SAMPLE_DATE'] <= modis_end)\n",
    "    ].copy()\n",
    "    \n",
    "    # create a UID \n",
    "    final_events['STABLE_EVENT_ID'] = (\n",
    "        final_events['SAMPLE_DATE'].dt.strftime('%Y%m%d') + '_' +\n",
    "        final_events['LATITUDE'].round(4).astype(str) + '_' +\n",
    "        final_events['LONGITUDE'].round(4).astype(str) + '_' +\n",
    "        final_events['CELLCOUNT'].astype(int).astype(str)\n",
    "    )\n",
    "    \n",
    "    # remove dupes based on ID\n",
    "    final_events = final_events.drop_duplicates(subset=['STABLE_EVENT_ID']).copy()\n",
    "    \n",
    "    # sort by date\n",
    "    final_events = final_events.sort_values('SAMPLE_DATE').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Total HAB events: {len(final_events):,}\")\n",
    "    print(f\"Positive: {len(final_events[final_events['HAB_EVENT'] == 1]):,}\")\n",
    "    print(f\"Negative: {len(final_events[final_events['HAB_EVENT'] == 0]):,}\")\n",
    "    print(f\"Dates range: {final_events['SAMPLE_DATE'].min()} to {final_events['SAMPLE_DATE'].max()}\")\n",
    "    \n",
    "    return final_events\n",
    "\n",
    "# create a balanced sample\n",
    "def create_mvp_sample(events_df, n_events=10):\n",
    "    print(f\"\\nCreating  sample of {n_events} events\")\n",
    "    \n",
    "    # use recent years 2015-2018\n",
    "    recent_events = events_df[events_df['SAMPLE_DATE'].dt.year >= 2015].copy()\n",
    "    \n",
    "    # class balance\n",
    "    n_positive = min(n_events // 2, len(recent_events[recent_events['HAB_EVENT'] == 1]))\n",
    "    n_negative = min(n_events // 2, len(recent_events[recent_events['HAB_EVENT'] == 0]))\n",
    "    \n",
    "    # random state for reruns\n",
    "    positive_sample = recent_events[recent_events['HAB_EVENT'] == 1].sample(\n",
    "        n=n_positive, random_state=42\n",
    "    )\n",
    "    negative_sample = recent_events[recent_events['HAB_EVENT'] == 0].sample(\n",
    "        n=n_negative, random_state=42\n",
    "    )\n",
    "    \n",
    "    mvp_sample = pd.concat([positive_sample, negative_sample], ignore_index=True)\n",
    "    mvp_sample = mvp_sample.sort_values('SAMPLE_DATE').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"MVP sample created: {len(mvp_sample)} events\")\n",
    "    print(f\"  Positive (HAB): {len(mvp_sample[mvp_sample['HAB_EVENT'] == 1])}\")\n",
    "    print(f\"  Negative (No HAB): {len(mvp_sample[mvp_sample['HAB_EVENT'] == 0])}\")\n",
    "    print(f\"  Date range: {mvp_sample['SAMPLE_DATE'].min()} to {mvp_sample['SAMPLE_DATE'].max()}\")\n",
    "    \n",
    "    return mvp_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96df1b08-7a15-49a7-9c8a-0a66f319b955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get MODIS L2 Ocean Color data\n",
    "def search_modis_l2_data(date, spatial_bounds):\n",
    "    bbox = (\n",
    "        spatial_bounds['lon_min'], spatial_bounds['lat_min'],\n",
    "        spatial_bounds['lon_max'], spatial_bounds['lat_max']\n",
    "    )\n",
    "    try:\n",
    "        granules = earthaccess.search_data(\n",
    "            short_name='MODISA_L2_OC',\n",
    "            temporal=(date.strftime('%Y-%m-%d'), date.strftime('%Y-%m-%d')),\n",
    "            bounding_box=bbox\n",
    "        )\n",
    "        return granules\n",
    "    except Exception as e:\n",
    "        print(f\"Search error for {date}: {e}\")\n",
    "        return []\n",
    "\n",
    "#  download satelite granule if not cached\n",
    "def download_and_cache_granule(granule, raw_dir):\n",
    "    granule_name = granule['umm']['GranuleUR']\n",
    "    cached_file = raw_dir / f\"{granule_name}.nc\"\n",
    "    \n",
    "    if cached_file.exists():\n",
    "        print(f\"- using cached file: {cached_file.name[:50]}\")\n",
    "        return str(cached_file)\n",
    "    \n",
    "    try:\n",
    "        print(f\" - downloading new file\")\n",
    "        files = earthaccess.download([granule], local_path=str(raw_dir))\n",
    "        if files and len(files) > 0:\n",
    "            downloaded_file = Path(files[0])\n",
    "            if downloaded_file != cached_file:\n",
    "                downloaded_file.rename(cached_file)\n",
    "            return str(cached_file)\n",
    "    except Exception as e:\n",
    "        print(f\" - download failed: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# get the modalities\n",
    "def extract_habnet_modalities_from_l2(file_path, spatial_bounds):\n",
    "    try:\n",
    "        with nc.Dataset(file_path, 'r') as ds:\n",
    "            # check if required groups exist\n",
    "            if 'geophysical_data' not in ds.groups or 'navigation_data' not in ds.groups:\n",
    "                print(f\" missing required groups in {Path(file_path).name}\")\n",
    "                return None\n",
    "\n",
    "            geo_data = ds.groups['geophysical_data']\n",
    "            nav_data = ds.groups['navigation_data']\n",
    "\n",
    "            # get coords\n",
    "            lats = nav_data.variables['latitude'][:]\n",
    "            lons = nav_data.variables['longitude'][:]\n",
    "\n",
    "            # spatial filtering\n",
    "            lat_mask = (lats >= spatial_bounds['lat_min']) & (lats <= spatial_bounds['lat_max'])\n",
    "            lon_mask = (lons >= spatial_bounds['lon_min']) & (lons <= spatial_bounds['lon_max'])\n",
    "            spatial_mask = lat_mask & lon_mask\n",
    "\n",
    "            if not np.any(spatial_mask):\n",
    "                print(f\" - no data in spatial bounds\")\n",
    "                return None\n",
    "\n",
    "            # get modalities\n",
    "            result = {\n",
    "                'lats': lats[spatial_mask],\n",
    "                'lons': lons[spatial_mask],\n",
    "                'modalities': {}\n",
    "            }\n",
    "\n",
    "            for modality in HABNET_MODALITIES:\n",
    "                if modality in geo_data.variables:\n",
    "                    mod_data = geo_data.variables[modality][:]\n",
    "                    if mod_data.shape == lats.shape:\n",
    "                        valid_data = mod_data[spatial_mask]\n",
    "                        \n",
    "                        # filter valid values\n",
    "                        if hasattr(valid_data, 'mask'):\n",
    "                            valid_mask = ~valid_data.mask\n",
    "                        else:\n",
    "                            valid_mask = np.isfinite(valid_data)\n",
    "\n",
    "                        # filter valid chlorophyll values \n",
    "                        if modality == 'chlor_a':\n",
    "                            valid_mask = valid_mask & (valid_data > 0) & (valid_data < 1000) \n",
    "\n",
    "                        if np.any(valid_mask):\n",
    "                            final_values = valid_data[valid_mask]\n",
    "                            result['modalities'][modality] = final_values\n",
    "                            print(f\" - {modality}: {np.sum(valid_mask)} points, \"\n",
    "                                  f\"range {np.min(final_values):.4f}-{np.max(final_values):.4f}\")\n",
    "\n",
    "            if result['modalities']:\n",
    "                return result\n",
    "            else:\n",
    "                print(f\"- no valid modality data\")\n",
    "                return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" - error with {Path(file_path).name}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0471f8-508c-4b0e-9893-38ea2635da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datacube pipeline \n",
    "class OptimizedHABNetDatacubeGenerator:\n",
    "    def __init__(self, raw_dir, processed_dir, config=DATACUBE_CONFIG):\n",
    "        self.raw_dir = Path(raw_dir)\n",
    "        self.processed_dir = Path(processed_dir)\n",
    "        self.config = config\n",
    "        self.search_cache = {}\n",
    "        \n",
    "# check if datacube already exists\n",
    "    def check_existing_datacube(self, stable_event_id):\n",
    "        output_file = self.processed_dir / f\"habnet_datacube_{stable_event_id}.pkl\"\n",
    "        return output_file.exists()\n",
    "\n",
    "    # check if we already have the nc file\n",
    "    def cached_search_modis_data(self, date, spatial_bounds):\n",
    "        cache_key = f\"{date.strftime('%Y-%m-%d')}_{spatial_bounds['lat_min']:.2f}_{spatial_bounds['lon_min']:.2f}\"\n",
    "        \n",
    "        if cache_key in self.search_cache:\n",
    "            return self.search_cache[cache_key]\n",
    "        \n",
    "        granules = search_modis_l2_data(date, spatial_bounds)\n",
    "        self.search_cache[cache_key] = granules\n",
    "        return granules\n",
    "\n",
    "# create new datacube\n",
    "    def generate_datacube_for_event(self, event_row):\n",
    "        stable_event_id = event_row['STABLE_EVENT_ID']\n",
    "        \n",
    "        # skip if already processed\n",
    "        if self.check_existing_datacube(stable_event_id):\n",
    "            output_file = self.processed_dir / f\"habnet_datacube_{stable_event_id}.pkl\"\n",
    "            print(f\"\\nDatacube for event {stable_event_id} already exists, skipping\")\n",
    "            return output_file\n",
    "\n",
    "        event_date = event_row['SAMPLE_DATE']\n",
    "        event_lat = event_row['LATITUDE']\n",
    "        event_lon = event_row['LONGITUDE']\n",
    "        hab_label = event_row['HAB_EVENT']\n",
    "\n",
    "        print(f\"\\nGenerating datacube for event: {stable_event_id}\")\n",
    "        print(f\"Date: {event_date.strftime('%Y-%m-%d')}, Location: ({event_lat:.3f}, {event_lon:.3f})\")\n",
    "        print(f\"HAB Event: {hab_label}\")\n",
    "\n",
    "        # spatial and temporal bounds\n",
    "        spatial_bounds = calculate_spatial_bounds(event_lat, event_lon, self.config['spatial_extent_km'])\n",
    "        temporal_bounds = calculate_temporal_bounds(event_date, self.config['temporal_extent_days'])\n",
    "\n",
    "        datacube_shape = (\n",
    "            self.config['spatial_extent_km'] // self.config['spatial_resolution_km'],\n",
    "            self.config['spatial_extent_km'] // self.config['spatial_resolution_km'],\n",
    "            self.config['temporal_extent_days']\n",
    "        )\n",
    "\n",
    "        # dict to store each modality\n",
    "        modality_datacubes = {}\n",
    "        for modality in HABNET_MODALITIES:\n",
    "            modality_datacubes[modality] = np.full(datacube_shape, np.nan)\n",
    "\n",
    "        successful_days = 0\n",
    "\n",
    "        # process each day in time window\n",
    "        for day_idx in range(self.config['temporal_extent_days']):\n",
    "            current_date = temporal_bounds['start_date'] + timedelta(days=day_idx)\n",
    "            print(f\"  Day {day_idx+1}/{self.config['temporal_extent_days']} ({current_date.strftime('%Y-%m-%d')}):\", end=\" \")\n",
    "\n",
    "            # check if we have cached MODIS data\n",
    "            granules = self.cached_search_modis_data(current_date, spatial_bounds)\n",
    "            if not granules:\n",
    "                print(f\"No data found\")\n",
    "                continue\n",
    "\n",
    "            # download and process first cached  granule\n",
    "            file_path = download_and_cache_granule(granules[0], self.raw_dir)\n",
    "            if not file_path:\n",
    "                print(f\"Download failed\")\n",
    "                continue\n",
    "\n",
    "            # process modalities\n",
    "            daily_data = extract_habnet_modalities_from_l2(file_path, spatial_bounds)\n",
    "            if daily_data and daily_data['modalities']:\n",
    "                # project each modality to grid\n",
    "                for modality, values in daily_data['modalities'].items():\n",
    "                    if len(values) > 0:\n",
    "                        gridded_data = self._reproject_to_regular_grid(\n",
    "                            daily_data['lats'], daily_data['lons'], values, spatial_bounds\n",
    "                        )\n",
    "                        modality_datacubes[modality][:, :, day_idx] = gridded_data\n",
    "\n",
    "                successful_days += 1\n",
    "                print(f\"SUCCESS: {len(daily_data['modalities'])} modalities processed\")\n",
    "            else:\n",
    "                print(f\"No valid modality data\")\n",
    "\n",
    "        # check data completeness\n",
    "        data_completeness = successful_days / self.config['temporal_extent_days']\n",
    "        print(f\"\\nDatacube generation complete: {successful_days}/{self.config['temporal_extent_days']} days ({data_completeness:.1%})\")\n",
    "\n",
    "        # only save if we have decent data completeness (4 days for now)\n",
    "        if data_completeness >= 0.8:\n",
    "            datacube_data = {\n",
    "                'datacubes': modality_datacubes,\n",
    "                'metadata': {\n",
    "                    'stable_event_id': stable_event_id,\n",
    "                    'date': event_date,\n",
    "                    'lat': event_lat,\n",
    "                    'lon': event_lon,\n",
    "                    'hab_label': hab_label,\n",
    "                    'cell_count': event_row['CELLCOUNT'],\n",
    "                    'spatial_bounds': spatial_bounds,\n",
    "                    'temporal_bounds': temporal_bounds,\n",
    "                    'config': self.config,\n",
    "                    'data_completeness': data_completeness,\n",
    "                    'successful_days': successful_days,\n",
    "                    'modalities': list(modality_datacubes.keys()),\n",
    "                    'generation_date': datetime.now()\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # save datacube\n",
    "            output_file = self.processed_dir / f\"habnet_datacube_{stable_event_id}.pkl\"\n",
    "            with open(output_file, 'wb') as f:\n",
    "                pickle.dump(datacube_data, f)\n",
    "\n",
    "            print(f\"Datacube saved: {output_file}\")\n",
    "            return output_file\n",
    "        else:\n",
    "            print(f\"Insufficient data ({data_completeness:.1%}) - datacube not saved\")\n",
    "            return None\n",
    "\n",
    "    \n",
    "    def _reproject_to_regular_grid(self, lats, lons, values, spatial_bounds):\n",
    "        # target grid\n",
    "        grid_size = self.config['spatial_extent_km'] // self.config['spatial_resolution_km']\n",
    "        target_lats = np.linspace(\n",
    "            spatial_bounds['lat_min'], spatial_bounds['lat_max'], grid_size\n",
    "        )\n",
    "        target_lons = np.linspace(\n",
    "            spatial_bounds['lon_min'], spatial_bounds['lon_max'], grid_size\n",
    "        )\n",
    "        target_lons_grid, target_lats_grid = np.meshgrid(target_lons, target_lats)\n",
    "\n",
    "        # get source points for interpolation\n",
    "        source_points = np.column_stack([lons.ravel(), lats.ravel()])\n",
    "        target_points = np.column_stack([target_lons_grid.ravel(), target_lats_grid.ravel()])\n",
    "\n",
    "        # remove invalid values\n",
    "        valid_mask = np.isfinite(values)\n",
    "        if np.sum(valid_mask) < 4:  # Need at least 4 points for triangulation\n",
    "            return np.full((grid_size, grid_size), np.nan)\n",
    "\n",
    "        source_points_valid = source_points[:len(values)][valid_mask]\n",
    "        source_values_valid = values[valid_mask]\n",
    "\n",
    "        try:\n",
    "            # interpolate to grid with lerp\n",
    "            interpolated = griddata(\n",
    "                source_points_valid, source_values_valid, target_points,\n",
    "                method='linear', fill_value=np.nan\n",
    "            )\n",
    "            \n",
    "            # fill leftoever NaNs with nearest neighbor if we can\n",
    "            if np.any(np.isnan(interpolated)) and len(source_values_valid) >= 1:\n",
    "                interpolated_nn = griddata(\n",
    "                    source_points_valid, source_values_valid, target_points,\n",
    "                    method='nearest', fill_value=np.nan\n",
    "                )\n",
    "                nan_mask = np.isnan(interpolated)\n",
    "                interpolated[nan_mask] = interpolated_nn[nan_mask]\n",
    "\n",
    "            gridded_data = interpolated.reshape(target_lons_grid.shape)\n",
    "            return gridded_data\n",
    "        except Exception as e:\n",
    "            print(f\"- interpolation failed: {e}\")\n",
    "            return np.full((grid_size, grid_size), np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbafcc7-3096-4d5d-b3da-d328bd79ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nasa earth auth\n",
    "def setup_nasa_earthdata():\n",
    "    print(\"Setting up NASA Earthdata auth\")\n",
    "    try:\n",
    "        auth = earthaccess.login()\n",
    "        if auth:\n",
    "            print(\" - NASA Earthdata auth successful!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\" - Auth failed\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"  Authentication error: {e}\")\n",
    "        return False\n",
    "\n",
    "def run_datacube_generation():\n",
    "    print(\"HAB detection Chl-a Datacube Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # setup directories\n",
    "    base_dir, raw_dir, processed_dir = setup_directories()\n",
    "    print(f\"Data directories created in: {base_dir}\")\n",
    "\n",
    "\n",
    "    # setup NASA authentication\n",
    "    if not setup_nasa_earthdata():\n",
    "        print(\"Cannot proceed without NASA Earthdata auth\")\n",
    "        return None\n",
    "\n",
    "    # load and filter HAB events\n",
    "    try:\n",
    "        hab_events = load_and_filter_hab_events()\n",
    "    except FileNotFoundError:\n",
    "        print(\"HAB events CSV file not found. Please ensure 'habsos_20240430.csv' is available.\")\n",
    "        return None\n",
    "\n",
    "    # create sample 1k for now\n",
    "    mvp_events = create_mvp_sample(hab_events, n_events=1000)\n",
    "\n",
    "    # Save MVP events for reference\n",
    "    mvp_file = base_dir / 'mvp_events.csv'\n",
    "    mvp_events.to_csv(mvp_file, index=False)\n",
    "    print(f\"MVP events saved to: {mvp_file}\")\n",
    "\n",
    "    # init optimized datacube generator\n",
    "    generator = OptimizedHABNetDatacubeGenerator(raw_dir, processed_dir)\n",
    "\n",
    "    print(f\"\\nDatacube Config:\")\n",
    "    print(f\"  Spatial: {generator.config['spatial_extent_km']}km x {generator.config['spatial_extent_km']}km\")\n",
    "    print(f\"  Temporal: {generator.config['temporal_extent_days']} days\")\n",
    "    print(f\"  Spatial resolution: {generator.config['spatial_resolution_km']}km\")\n",
    "    print(f\"  Modalities: {HABNET_MODALITIES}\")\n",
    "\n",
    "    # guess processing time\n",
    "    base_time_per_event = 1.0  # about a min per event\n",
    "    estimated_total = len(mvp_events) * base_time_per_event\n",
    "\n",
    "    # Generate datacubes for events\n",
    "    print(f\"\\nGenerating datacubes for {len(mvp_events)} MVP events...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results = []\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    for idx, (event_idx, event) in enumerate(mvp_events.iterrows()):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing event {idx+1}/{len(mvp_events)}: ID {event_idx}\")\n",
    "        \n",
    "        event_start = datetime.now()\n",
    "        try:\n",
    "            output_file = generator.generate_datacube_for_event(event)\n",
    "            event_time = (datetime.now() - event_start).total_seconds() / 60\n",
    "            \n",
    "            results.append({\n",
    "                'event_id': event_idx,\n",
    "                'hab_label': event['HAB_EVENT'],\n",
    "                'output_file': output_file,\n",
    "                'success': output_file is not None,\n",
    "                'processing_time_min': event_time\n",
    "            })\n",
    "            \n",
    "            print(f\"Event completed in {event_time:.1f} minutes\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            event_time = (datetime.now() - event_start).total_seconds() / 60\n",
    "            print(f\"Failed to process event {event_idx}: {e}\")\n",
    "            results.append({\n",
    "                'event_id': event_idx,\n",
    "                'hab_label': event['HAB_EVENT'],\n",
    "                'output_file': None,\n",
    "                'success': False,\n",
    "                'processing_time_min': event_time,\n",
    "                'error': str(e)\n",
    "            })\n",
    "\n",
    "    # summary\n",
    "    total_time = (datetime.now() - start_time).total_seconds() / 60\n",
    "    successful_results = [r for r in results if r['success']]\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"DATACUBE GENERATION SUMMARY\")\n",
    "    print(f\"Total processing time: {total_time:.1f} minutes\")\n",
    "    print(f\"Average time per event: {total_time/len(results):.1f} minutes\")\n",
    "    print(f\"Total events processed: {len(results)}\")\n",
    "    print(f\"Successful: {len(successful_results)}\")\n",
    "    print(f\"Failed: {len(results) - len(successful_results)}\")\n",
    "\n",
    "    if successful_results:\n",
    "        print(f\"\\nSuccessful datacubes saved in: {processed_dir}\")\n",
    "    return results, base_dir\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting HABNet MVP Pipeline\")\n",
    "    print(f\"Configuration: {DATACUBE_CONFIG}\")\n",
    "    print(f\"Modalities: {HABNET_MODALITIES}\")\n",
    "\n",
    "    results, data_dir = run_datacube_generation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
